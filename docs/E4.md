
# Partie E4: Réalisation d'un service numérique

**<ins>Objet</ins>**: Cette partie a pour objet l'exposition par API des données collectées via plusieurs sources: fichier plat, API, scraping.

## Structure de la partie:

```
.
├── E4/
|   ├── harmonie/
|       ├── BDD/                            traitement des données
|           ├── pages/                      pages du front
|               ├── 1_auteurs
|               ├── 2_partitions
|               ├── 3_événements
|               ├──
|           ├──routes/                      contrôleurs de l'API
|               ├── associations.py
|               ├── auteurs.py
|               ├── authenification.py
|               ├── evenements.py
|               ├── partitions_hbm.py
|               └── users.py
|           ├── __init__.py
|           ├── .env
|           ├── api_externe.py              extraction des données de l'API musicbrainz.org
|           ├── api_hbm.py                  exposition des données
|           ├── app_streamlit.py            script principal de streamlit
|           ├── auth.py
|           ├── config.py                   fichier de configuration
|           ├── create_db.py                script d'insertion dans la base données
|           ├── crud.py                     create, read, update, delete pour script ou api
|           ├── database.py                 script de connexion aux bases de données
|           ├── models.py                   définition des classes SQLalchemy
|           └── schemas.py                  définition des classes Pydantic
|
|       ├── harmonie/                       scraping 
|           ├── archives/                   dossier d'archives des scrapings
|               ├── musicshop_2025-02-27.csv
|               └── musicshop_2025-07-02.csv
|           ├── spiders/
|               ├── __init__.py
|               └── hbm_scrap.py            spider de scraping
|           ├── __init__.py
|           ├── clean_scrapy.py             alimentation de deux fichiers: nouvelles entrées et cumul       
|           ├── items.py                    définition de la classe d'items scrapés 
|           ├── middlewares.py
|           ├── musicshop_all.csv           fichier cumul des scrapings
|           ├── musicshop_last.csv          fichier du dernier scraping
|           ├── musicshop_new.csv           fichier des nouveautés 
|           ├── pipelines.py                nettoyage des items scrapés
|           ├── settings.py                 paramètres du projet scraping
|       ├── scrapy.cfg
|       └── schema.md                       schéma mermaid base de données
| RGPD_fake.ods                             traitements RGPD anonymisés

``` 
## Dépendances:

Ce projet nécessite:
scrapy, dateparser, pandas, sqlachemy, dotenv, pydantic, fastapi, (streamlit), uvicorn, python-mutipart, pyjwt et passlib.  
Le détail des versions se trouve dans le fichier pyproject.toml.  
J'ai pour ma part utilisé poetry pour créer mon environnement.

## Description des sous-parties:

***

### Le scraping

J'ai utilisé Scrapy pour scraper le site musicshopeurope.fr, j'ai pris comme critères de sélection la formation (orchestre d'harmonie) et les partitions comprenant un conducteur.  
Cette sous-partie du projet est dans le répertoire E4/harmonie/harmonie.  
La documentation de Scrapy se trouve à l'adresse: [https://www.scrapy.org](https://www.scrapy.org)
  
Pour initier le projet scrapy, il faut exécuter la commande *scrapy startproject harmonie*, se placer à sa racine (même niveau que scrapy.cfg) et exécuter la seconde commande *scrapy genspider hbm_scrap musicshopeurope.fr*.  
Un dossier harmonie contenant les fichiers items.py, middelware.py, pipelines.py et settings.py se crée ainsi qu'un sous-dossier contenant hbm_scrap.py.  
  
Le script principal est hbm_scrap.py. La méthode parse() permet la navigation jusqu'aux pages scrapées tandis que parse_partition() en cible les éléments html. On obtient un objet item instance de la classe HarmonieItem définie dans items.py.   
Je n'ai pas modifié le fichier middleware.py.  
Dans settings.py, il faut compléter le USER_AGENT avec ses propres informations.  
Le pipelines.py permet de nettoyer les données extraites et éventuellement de les insérer en base de données.  
Le temps d'exécution pour ce projet étant très important, j'ai préféré scraper des blocs de 100 pages (1h de scraping environ) et ne pas utiliser de pipeline d'insertion.  

Pour démarrer le scraping, se placer au niveau du dossier parent du spider et exécuter  
*scrapy crawl hbm_scrap -O musicshop_last.csv*  
Le nom de fichier *musicshop_last.csv* est important puisqu'il est utlisé dans la suite du processus. 

Du fait du choix d'une plage de pages fixe à scraper (10 couvrant plusieurs mois de nouveautés), le fichier musicshop_last.csv contient des doublons. Pour éviter de les traiter au moment de l'insertion en base de données, on effectue un traitement: on compare les deux fichiers, les différences (ajouts) sont insérées dans un fichier *musicshop_AAAA-MM-DD.csv*, enregistré dans le dossier archives, ainsi que dans le fichier *musicshop_all.csv* qui constitue le cumul des entrées.  
Le fichier musicshop_last.csv est conservé en l'état jusqu'au prochaine scraping et le fichier musicshop_new.csv ne contient que les nouveautés qui seront insérées.  

Pour effectuer ce traitement, on exécute *clean_scrapy.py*.  

***

### L'API externe

Parmi les sources requises pour ce projet fingure une API. J'ai choisi celle de MusicBrainz.org.  
Le site musicshopeurope.fr ne permet pas de scraper séparément le nom et le prénom d'un auteur. L'API récupère cette information ainsi que d'autres complémentaires à partir du nom complet.  
Dans le script *api_externe.py*, la fonction get_api_externe() fait ce traitement.   
Cette sous-partie du projet est dans le répertoire E4/harmonie/BDD.


***

### La BDD

J'ai construit la partie base de données sur SQLAlcheny, permettant ainsi de changer de SGBDR sans grosse modification du code. Dans l'état actuel, la BDD créée est une base SQLite locale.    
Cette sous-partie du projet est dans le répertoire E4/harmonie/BDD.  
La documentation SQLAlchemy se trouve à l'adresse: [https://www.sqlalchemy.org/](https://www.sqlalchemy.org/)
  
Les scripts s'organisent ainsi:  

+ *database.py* établit les connexions aux bases de données et crée les sessions.
+ *models.py* définit les tables de la BDD
+ *crud.py* contient toutes les fonctions CRUD qui peuvent être exécutées directement mais qui sont aussi appelées par l'API ou encore le script d'alimentation de la BDD
+ *create_db.py* contient les scripts d'alimentation de la BDD à partir de fichiers .csv. La fonction insert_event_to_db() insère les événements dans la table du même nom, insert_scrapy_to_db() insère les données venant du scraping mais également de l'API musicbrainz.org, insert_users_to_db() insère de faux users pour tester l'API.  

Pour créer la BDD et l'alimenter avec les données de démo, il suffit d'exécuter le script *create_db.py* au niveau de E4/harmonie/BDD/  

***

### L'API

L'API REST est développée avec FastAPI. Documentation à l'adresse [https://fastapi.tiangolo.com/learn/](https://fastapi.tiangolo.com/learn/).   
Cette sous-partie du projet se trouve dans le répertoire E4/harmonie/BDD.  
Elle comprend les fichiers:
+ *api_hbm.py*. C'est le fichier principal qui appelle les contrôleurs de routes/
+ *schemas.py*. Les classes Pydantic qui y sont déclarées permettent la validation des types de données et le formatage des réponses.
+ *auth.py* contient les fonctions nécessaires pour mettre en place l'authentification. 3 variables sont définies dans le .env.  
SECRET_KEY = "09d25e094faa6ca2556c818166b7a9563b93f7099f6f0f4caa6cf63b88e8d3e7"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30  
Cette clé secrête est celle de la documentation FastAPI.  Pour générer votre propre clé, exécuter la commande *openssl rand -hex 32* dans un terminal.  

ainsi que le dossier routes/ qui lui-même contient:
+ *associations.py*
+ *auteurs.py*
+ *authentification.py*
+ *evenements.py*
+ *partitions_hbm.py*
+ *partitions.py
+ *users.py*  
Chacun de ces fichiers contient toutes les fonctions d'API se rapportant au contrôleur du même nom.  

Pour démarrer l'API, il faut exécuter *uvicorn api_hbm:app* au niveau de E4/harmonie/BDD/  
Elle est visible pour l'instant à l'adresse [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) ou [http://127.0.0.1:8000/redoc](http://127.0.0.1:8000/redoc).

***

### Registre RGPD

Le registre de traitements simplifié contient la liste des données sensibles et leur traitement.


